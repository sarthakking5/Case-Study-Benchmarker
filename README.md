# Manufacturing Case Study Benchmarker

A template framework to standardize manufacturing datasets (e.g., FJSSP), create reproducible case studies, and benchmark algorithms with built-in baseline solvers and an evaluation environment.

- âœ… CSV/JSON ingestion â†’ cleaned, standardized schema (Pydantic)

- âœ… Baseline solvers (Random, Greedy; optional OR-Tools placeholder)

- âœ… CLI (runner.py) to standardize, list, and run benchmarks

- âœ… End-to-end pipeline script (standardize â†’ list â†’ run)

- âœ… Ready for researchers to plug in custom algorithms

## ğŸ“¦ Repository Structure
 ``` graphql

case_study_benchmarker/
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .gitignore
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # (optional) keep your raw uploads
â”‚   â”œâ”€â”€ standardized/       # generated standardized cases (JSON)
â”‚   â”œâ”€â”€ results/            # solver outputs / logs (if you add persistence)
â”‚   â”œâ”€â”€ sample_case.csv     # sample input
â”‚   â””â”€â”€ sample_case.json    # sample input
â”‚
â”œâ”€â”€ loaders/
â”‚   â”œâ”€â”€ csv_loader.py       # CSV â†’ standardized dict (handles missing values)
â”‚   â”œâ”€â”€ json_loader.py      # flexible JSON â†’ standardized dict
â”‚   â””â”€â”€ validator.py        # Pydantic schema + validate_case()
â”‚
â”œâ”€â”€ case_study/
â”‚   â”œâ”€â”€ generator.py        # save validated case to data/standardized/
â”‚   â”œâ”€â”€ registry.py         # list/load standardized cases
â”‚   â””â”€â”€ exporter.py         # convenience: flat table export
â”‚
â”œâ”€â”€ solvers/
â”‚   â”œâ”€â”€ random_solver.py    # baseline: random assignment
â”‚   â”œâ”€â”€ greedy_solver.py    # baseline: min processing time per op
â”‚   â”œâ”€â”€ ortools_solver.py   # optional placeholder (replace with CP-SAT)
â”‚   â””â”€â”€ evaluator.py        # discrete-event style simulator + KPIs
â”‚
â”œâ”€â”€ environment/
â”‚   â”œâ”€â”€ benchmark_env.py    # API: run baselines or custom solvers
â”‚   â””â”€â”€ runner.py           # CLI: standardize / list / run
â”‚
â””â”€â”€ examples/
    â””â”€â”€ run_full_pipeline.py # end-to-end demo using the CLI internally
```
## ğŸ§± Standardized Data Schema (Pydantic)
``` json
{
  "case_id": "string",
  "metadata": {
    "source": "string",
    "objective": "Minimize Makespan",
    "created_by": "Campus Heilbronn"
  },
  "jobs": [
    {
      "job_id": "J1",
      "operations": [
        {
          "operation_id": "O1",
          "machines": [
            { "id": "M1", "time": 10.0 },
            { "id": "M2", "time": 12.0 }
          ]
        }
      ]
    }
  ]
}

```
### Notes
- `time` may be null (missing /invalid in raw uploads is tolerated)
- Critical IDs (JobID, OperationID, MachineID) are required; rows missing these are dropped during CSV processing.

## ğŸ“¥ Input Data Formats

### CSV (expected columns)
``` python-repl
JobID,OperationID,MachineID,ProcessingTime
J1,O1,M1,10
J1,O1,M2,12
J1,O2,M1,8
...
```
### Flexible JSON (auto normalized)
Matches `data/sample_case.json (both standardized and loose formats are accepted).

## âš™ï¸ Installation
``` bash
# from repo root
python -m venv venv
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate

pip install -r requirements.txt
```
## How to Run(CLI)
All commands are run from repo root. The CLI lives at `environment/runner.py`.

### 1) Standardize raw CSV/JSON

``` bash 
python environment/runner.py standardize data/sample_case.csv --case_id TestCase_001
# or
python environment/runner.py standardize data/sample_case.json --case_id TestCase_JSON
```
Outputs a validated JSON case at:
`data/standardized/<case_id>.json`

### 2) List standardized cases
```bash
python environment/runner.py list
```
### 3) Run baselines on a case
``` bash 
python environment/runner.py run data/standardized/TestCase_001.json
```
#### Example output:
```lua 
solver                 makespan    num_tasks
-------------------  ----------  ----------
RandomSolver               17.00           4
GreedyMinProcTime          12.00           4
ORToolsHeuristic(min-time) 12.00           4
```

## ğŸ”„ End-to-End (one command)
Use the included pipeline script (it ensures the venv python is used):
``` bash
python examples/run_full_pipeline.py
```
### Example Output 
``` lua
â¡ï¸  Step 1: Standardizing raw CSV/JSON...
âœ… Saved standardized case: data\standardized\TestCase_FullPipeline.json

â¡ï¸  Step 2: Listing standardized cases...
case_path
--------------------------------------------
data\standardized\MyTestCase.json
data\standardized\TestCase_FullPipeline.json

â¡ï¸  Step 3: Running baseline solvers on data/standardized/TestCase_FullPipeline.json...
solver                        makespan    num_tasks
--------------------------  ----------  -----------
RandomSolver                     12.00            4
GreedyMinProcTime                12.00            4
ORToolsHeuristic(min-time)       12.00            4
```

## ğŸ§  Adding Your Own Solver
``` python
# my_solver.py
def solve(case: dict) -> dict:
    assignments = []
    for job in case["jobs"]:
        for op in job["operations"]:
            # pick any machine you want (heuristics/ML/CP-SAT)
            chosen = min(
                [m for m in op["machines"] if m["time"] is not None],
                key=lambda m: m["time"],
                default=None
            )
            if chosen:
                assignments.append({
                    "job_id": job["job_id"],
                    "operation_id": op["operation_id"],
                    "machine_id": chosen["id"]
                })
    return {"name": "MySolver", "assignments": assignments}
```

 Evaluate it:
``` python 
from environment.benchmark_env import BenchmarkEnv
env = BenchmarkEnv("data/standardized/TestCase_001.json")
print(env.run_custom_solver(lambda c: solve(c)))
```

##  ğŸ“Š What the Evaluator Computes

The simulator in `solvers/evaluator.py`:

- Enforces job precedence (O1 â†’ O2 â†’ â€¦ within a job)
- Unary machine capacity (one operation at a time)
- Computes start/finish times, makespan, and machine utilization
- Ignores operations with missing processing time (`time=None`)

## ğŸ§¾ Characteristics of This Repository

- Reproducible: All standardized cases follow a single schema (Pydantic validated).
- Extensible: Add new solvers by implementing a `solve(case) -> {"name", "assignments": [...]}` function.
- Practical: Baselines + simulation give instant KPIs (makespan/utilization).
- Robust to messy data: Missing `ProcessingTime` handled as None; invalid rows dropped if they lack IDs.
- Research-friendly: Exporters/registry make it easy to share case studies and compare algorithms.

## Roadmap 

- CP-SAT/OR-Tools full FJSSP model
- Stochastic processing times & robustness tests
- Multi-objective KPIs (tardiness, flow time)
- Streamlit UI for drag-and-drop + live KPIs
- Docker image for zero-setup reproduction

### Questions or issues? Open an issue or ping the maintainer. Happy benchmarking! ğŸ¯
